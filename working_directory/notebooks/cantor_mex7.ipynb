{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc93861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, math, json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Config, GPT2LMHeadModel, PreTrainedTokenizerFast,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a62761",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_LANG = \"Python\"  # choose a topic/language slice\n",
    "MAX_DOCS   = 1000      # per instructions (use smaller for quick smoke-test)\n",
    "TRAIN_FRAC = 0.9\n",
    "SEQ_LEN    = 256\n",
    "WP_VOCAB   = 16000\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 1         # bump to 2–3 if you have time/GPU\n",
    "LR         = 3e-4\n",
    "N_LAYERS   = 4; N_HEADS = 4; N_EMBD = 256\n",
    "SEED       = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f895f155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fa661bde63479b8d8a2fba1bc7c2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8b2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de178fe7d804550a368c8393eb00f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 12ffa550-8b9e-450b-912a-22c6c2c0e652)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x720f675f5340>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 94f0c85f-864c-473a-8c97-f144d53436fb)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x720f64185190>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 857eae8e-73a7-44fd-a9d4-3c4ee974a6c6)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x720f6733c500>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: f333bb4b-cd03-4de5-a53f-fbb59d39f73a)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x720f675f6ae0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 88551241-7049-4a9d-85bd-e66d917c00f1)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Got disconnected from remote data host. Retrying in 5sec [1/20]\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b7e38911-f1c4-43aa-9772-19236bf64546)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7e7c2503-ece1-4797-82cc-0b4ecf648bf3)')' thrown while requesting GET https://huggingface.co/datasets/bigcode/the-stack-v2/resolve/7408bfbcfd48e5833d62fd3dba48afd20d109473/data/Python/train-00007-of-00009.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No docs found. Check language name or dataset fields.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo docs found. Check language name or dataset fields.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m texts\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m docs = \u001b[43mload_stack_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTOPIC_LANG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_DOCS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs), docs[\u001b[32m0\u001b[39m][:\u001b[32m200\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_stack_sample\u001b[39m\u001b[34m(lang, n_docs)\u001b[39m\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m texts:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo docs found. Check language name or dataset fields.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m texts\n",
      "\u001b[31mRuntimeError\u001b[39m: No docs found. Check language name or dataset fields."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_stack_sample(lang: str, n_docs: int):\n",
    "    \"\"\"\n",
    "    Downloads (or reuses cached) subset of The Stack v2 for a given language.\n",
    "    No streaming—this will store a small shard on disk so you can re-run offline.\n",
    "    \"\"\"\n",
    "    # Try the language-specific configuration if available\n",
    "    try:\n",
    "        ds = load_dataset(\"bigcode/the-stack-v2\", lang, split=\"train[:{}]\".format(n_docs))\n",
    "    except Exception as e:\n",
    "        print(\"Language subset not found or dataset too large, falling back to filtered version.\", e)\n",
    "        ds = load_dataset(\"bigcode/the-stack-v2\", split=\"train[:50000]\")  # fallback: small slice\n",
    "        ds = ds.filter(lambda r: (r.get(\"lang\") or r.get(\"language\") or \"\").lower() == lang)\n",
    "        ds = ds.select(range(min(n_docs, len(ds))))\n",
    "    \n",
    "    texts = []\n",
    "    for row in ds:\n",
    "        text = row.get(\"content\") or row.get(\"text\") or \"\"\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "        if len(texts) >= n_docs:\n",
    "            break\n",
    "    if not texts:\n",
    "        raise RuntimeError(\"No docs found. Check language name or dataset fields.\")\n",
    "    return texts\n",
    "\n",
    "docs = load_stack_sample(TOPIC_LANG, MAX_DOCS)\n",
    "len(docs), docs[0][:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordpiece(texts, vocab_size=16000):\n",
    "    tok = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tok.pre_tokenizer = Whitespace()\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size, min_frequency=2,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    tok.train_from_iterator(texts, trainer=trainer)\n",
    "    tok.post_processor = TemplateProcessing(\n",
    "        single=\"$A\",\n",
    "        pair=\"$A [SEP] $B:1\",\n",
    "        special_tokens=[(\"[SEP]\", tok.token_to_id(\"[SEP]\"))],\n",
    "    )\n",
    "    fast = PreTrainedTokenizerFast(tokenizer_object=tok)\n",
    "    fast.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    return fast\n",
    "\n",
    "wp_tok = train_wordpiece(docs, WP_VOCAB)\n",
    "len(wp_tok), wp_tok.tokenize(\"def hello_world(): pass\")[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca5950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blob_id': 'd44bbb217114c0831167824d694d57c29ab86665', 'directory_id': 'e3f3f911019ac126d01c056eafc7c3183107a5af', 'path': '/Traffic Sign Detection/all_signs_combined/src/predict.py', 'content_id': '19ed9a428015b625610be9930dfee35938fb451b', 'detected_licenses': [], 'license_type': 'no_license', 'repo_name': 'uncctrafficsigndetection/Traffic-Sign-Detection', 'snapshot_id': '595258766f865c4b3c628b002d7b93a774168a9b', 'revision_id': '3ff4be52357f4b6340fef94124f8c835ab66fd8a', 'branch_name': 'refs/heads/master', 'visit_date': Timestamp('2020-04-09 20:28:33.910961'), 'revision_date': Timestamp('2018-12-05 21:29:50'), 'committer_date': Timestamp('2018-12-05 21:29:50'), 'github_id': 160574509, 'star_events_count': 0, 'fork_events_count': 0, 'gha_license_id': None, 'gha_event_created_at': None, 'gha_created_at': None, 'gha_language': None, 'src_encoding': 'UTF-8', 'language': 'Python', 'is_vendor': False, 'is_generated': False, 'length_bytes': 959, 'extension': 'py'}\n"
     ]
    }
   ],
   "source": [
    "split = int(len(docs) * TRAIN_FRAC)\n",
    "train_texts, val_texts = docs[:split], docs[split:]\n",
    "\n",
    "raw = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts}),\n",
    "    \"validation\": Dataset.from_dict({\"text\": val_texts})\n",
    "})\n",
    "\n",
    "def chunkify(examples, tokenizer, max_length=SEQ_LEN):\n",
    "    ids = tokenizer(examples[\"text\"], add_special_tokens=False).input_ids\n",
    "    flat = [i for doc in ids for i in doc]\n",
    "    chunks = [flat[i:i+max_length] for i in range(0, len(flat)-max_length, max_length)]\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "ds_train = raw[\"train\"].map(lambda ex: chunkify(ex, wp_tok), batched=True, remove_columns=[\"text\"]).flatten_indices()\n",
    "ds_val   = raw[\"validation\"].map(lambda ex: chunkify(ex, wp_tok), batched=True, remove_columns=[\"text\"]).flatten_indices()\n",
    "\n",
    "len(ds_train), len(ds_val), ds_train[0][\"input_ids\"][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"me7_wp_decoder_ckpt\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg = GPT2Config(\n",
    "    vocab_size=len(wp_tok),\n",
    "    n_positions=SEQ_LEN, n_ctx=SEQ_LEN,\n",
    "    n_embd=N_EMBD, n_layer=N_LAYERS, n_head=N_HEADS,\n",
    "    bos_token_id=wp_tok.cls_token_id or wp_tok.pad_token_id,\n",
    "    eos_token_id=wp_tok.sep_token_id or wp_tok.pad_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(cfg)\n",
    "model.resize_token_embeddings(len(wp_tok))\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=wp_tok, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",            # <— save a checkpoint each epoch\n",
    "    save_total_limit=3,               # <— keep last 3 to save disk\n",
    "    load_best_model_at_end=True,      # <— restores best on eval_loss at end\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=ds_train, eval_dataset=ds_val,\n",
    "    data_collator=collator, tokenizer=wp_tok\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "trainer.train()           # first run — from scratch\n",
    "decoder_train_s = time.time() - t0\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "decoder_loss = metrics[\"eval_loss\"]\n",
    "decoder_ppl  = math.exp(decoder_loss)\n",
    "decoder_train_s, decoder_ppl, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer & best model to OUTPUT_DIR/final\n",
    "final_dir = OUTPUT_DIR / \"final\"\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "wp_tok.save_pretrained(str(final_dir))\n",
    "trainer.model.save_pretrained(str(final_dir))\n",
    "print(\"Saved:\", final_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
