{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf8e2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# NLP related packages\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf190d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labels: 173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2019 ncov complete genome sequences',\n",
       " '2019 ncov genome sequence',\n",
       " '2019 ncov genome sequences',\n",
       " '2019ncov complete genome sequences',\n",
       " '2019ncov genome sequence',\n",
       " '2019ncov genome sequences',\n",
       " 'adni',\n",
       " 'advanced national seismic system anss comprehensive catalog comcat',\n",
       " 'advanced national seismic system comprehensive catalog',\n",
       " 'aging integrated database']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic text cleaning\n",
    "def text_cleaning(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra spaces\n",
    "    text = ''.join([k for k in text if k not in string.punctuation])  # remove punctuation\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', str(text).lower()).strip()   # alnum + lowercase\n",
    "    return text\n",
    "\n",
    "# A lighter cleaning for sentences (preserve punctuation tokens for output, normalize for matching)\n",
    "def light_clean(text: str) -> str:\n",
    "    # normalize spaces only; keep punctuation for output tokenization\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def normalize_token(t: str) -> str:\n",
    "    # per-token normalization for matching: strip punct, lowercase\n",
    "    t = t.strip().lower()\n",
    "    t = re.sub(r\"^\\W+|\\W+$\", \"\", t)\n",
    "    return t\n",
    "\n",
    "\n",
    "# Expect columns: dataset_label, dataset_title, cleaned_label\n",
    "df_train = pd.read_csv(\"../data/mex2/train.csv\")\n",
    "\n",
    "def _safe_clean_series(s: pd.Series):\n",
    "    return [text_cleaning(x) for x in s.fillna(\"\").astype(str)]\n",
    "\n",
    "temp_1 = _safe_clean_series(df_train['dataset_label'])\n",
    "temp_2 = _safe_clean_series(df_train['dataset_title'])\n",
    "temp_3 = _safe_clean_series(df_train['cleaned_label'])\n",
    "\n",
    "existing_labels = set([x for x in (temp_1 + temp_2 + temp_3) if x])\n",
    "\n",
    "print(f\"Loaded labels: {len(existing_labels):,}\")\n",
    "# Quick peek\n",
    "list(sorted(list(existing_labels))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d1878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def find_longest_label_in_sentence(clean_sentence_for_match: str,\n",
    "                                   labels_cleaned: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Returns the *cleaned* longest label whose string appears in the cleaned sentence.\n",
    "    Uses your STEP-1 cleaning (case-insensitive).\n",
    "    \"\"\"\n",
    "    longest = \"\"\n",
    "    for lab in labels_cleaned:\n",
    "        if lab and lab in clean_sentence_for_match:\n",
    "            if len(lab) > len(longest):\n",
    "                longest = lab\n",
    "    return longest\n",
    "\n",
    "def schwartz_hearst_abbrev(sent: str):\n",
    "    \"\"\"\n",
    "    Extract long form + short form candidates using Schwartz–Hearst pattern.\n",
    "    Returns list of (long_form, short_form).\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    pattern = re.compile(r\"\\(([^)]+)\\)\")\n",
    "    for match in pattern.finditer(sent):\n",
    "        short = match.group(1).strip()\n",
    "        start = match.start()\n",
    "        long = sent[:start].strip().split()\n",
    "        if not long:\n",
    "            continue\n",
    "        # Take a window before the parenthesis\n",
    "        window = \" \".join(long[-len(short)*2:])  # heuristic\n",
    "        candidates.append((window, short))\n",
    "    return candidates\n",
    "\n",
    "def bio_tags_for_sentence(tokens: List[str], label_clean: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given original tokens (for output) and a single cleaned label string, produce BIO tags.\n",
    "    - Matching is done on normalized tokens (lowercased, stripped punctuation).\n",
    "    - Only the first occurrence of the label is tagged (consistent with your earlier logic).\n",
    "    \"\"\"\n",
    "    if not label_clean:\n",
    "        return [\"O\"] * len(tokens)\n",
    "\n",
    "    # Tokenize the label in its cleaned form (split on spaces)\n",
    "    label_tokens_clean = [t for t in label_clean.split() if t]\n",
    "    if not label_tokens_clean:\n",
    "        return [\"O\"] * len(tokens)\n",
    "\n",
    "    sent_norm = [normalize_token(t) for t in tokens]\n",
    "    # Build a normalized version of the label tokens (they're already cleaned)\n",
    "    lab_norm = label_tokens_clean\n",
    "\n",
    "    n, m = len(sent_norm), len(lab_norm)\n",
    "    if m > n:\n",
    "        return [\"O\"] * n\n",
    "\n",
    "    # Sliding-window match\n",
    "    span_start = -1\n",
    "    for i in range(0, n - m + 1):\n",
    "        if sent_norm[i:i+m] == lab_norm:\n",
    "            span_start = i\n",
    "            break\n",
    "\n",
    "    tags = [\"O\"] * n\n",
    "    if span_start != -1:\n",
    "        tags[span_start] = \"B\"\n",
    "        for j in range(1, m):\n",
    "            tags[span_start + j] = \"I\"\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8837a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_jsonl(jsonl_path: str):\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86bc8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_conll(jsonl_path: str,\n",
    "                   out_path: str,\n",
    "                   labels_clean_set: set) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Reads a JSONL with at least 'text' (plus your file_name, n_sections) and writes CoNLL:\n",
    "        TOKEN<TAB>TAG\n",
    "    Blank line between sentences.\n",
    "    Returns: (n_docs, n_sents, n_sents_with_BIO)\n",
    "    \"\"\"\n",
    "    n_docs = n_sents = n_tagged = 0\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as w:\n",
    "        for row in tqdm(iter_jsonl(jsonl_path), desc=\"Docs\"):\n",
    "            n_docs += 1\n",
    "            text = (row.get(\"text\") or \"\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Sentence splitting on the *lightly* cleaned text to avoid random breaks\n",
    "            sentences = sent_tokenize(light_clean(text))\n",
    "            \n",
    "            for sent in sentences:\n",
    "                tokens = word_tokenize(sent)\n",
    "\n",
    "                # Candidate labels for this sentence\n",
    "                candidates = []\n",
    "\n",
    "                # 1) Gazetteer-based (longest match from known labels)\n",
    "                clean_sent_for_match = text_cleaning(sent)   # STEP-1 cleaner\n",
    "                longest_label_clean = find_longest_label_in_sentence(\n",
    "                    clean_sent_for_match, labels_clean_set\n",
    "                )\n",
    "                if longest_label_clean:\n",
    "                    candidates.append(longest_label_clean)\n",
    "\n",
    "                # 2) Schwartz–Hearst detection (long-form + short-form acronyms)\n",
    "                for lf, sf in schwartz_hearst_abbrev(sent):\n",
    "                    lf_clean = text_cleaning(lf)\n",
    "                    sf_clean = text_cleaning(sf)\n",
    "                    if lf_clean:\n",
    "                        candidates.append(lf_clean)\n",
    "                    if sf_clean:\n",
    "                        candidates.append(sf_clean)\n",
    "\n",
    "                # Pick the longest candidate among all\n",
    "                max_lab = max(candidates, key=len) if candidates else \"\"\n",
    "\n",
    "                # Tag sentence\n",
    "                tags = bio_tags_for_sentence(tokens, max_lab)\n",
    "\n",
    "                if \"B\" in tags:\n",
    "                    n_tagged += 1\n",
    "                n_sents += 1\n",
    "\n",
    "                # Write CoNLL lines\n",
    "                for tok, tag in zip(tokens, tags):\n",
    "                    w.write(f\"{tok}\\t{tag}\\n\")\n",
    "                w.write(\"\\n\")  # sentence boundary\n",
    "\n",
    "    return n_docs, n_sents, n_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23c81d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 1000it [01:34, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "  Docs: 1000\n",
      "  Sentences: 346535\n",
      "  Sentences with BIO tags: 25878\n",
      "  Saved: ../data/mex6/conll_1000.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "JSONL_IN  = \"../data/mex3/corpus_1000.jsonl\"\n",
    "CONLL_OUT = \"../data/mex6/conll_1000.conll\"\n",
    "\n",
    "n_docs, n_sents, n_tagged = jsonl_to_conll(JSONL_IN, CONLL_OUT, existing_labels)\n",
    "print(f\"Done.\\n  Docs: {n_docs}\\n  Sentences: {n_sents}\\n  Sentences with BIO tags: {n_tagged}\\n  Saved: {CONLL_OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
